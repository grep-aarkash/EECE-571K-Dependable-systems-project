\section{Related Work}
Reliability and safety of individual components in ADS has been previously studied. Li et al.~\cite{guanpeng17sc} studied the impact of soft errors on DNN using Fault injections on hardware accelerators and proposed solutions for making DNNs more resilient. Using Fault injections for testing reliability of system has inherent disadvantage that for complex systems like DNNs a huge number of FI are required to get statistically meaningful insights, moreover the coverage that FI provides is also dependent on the input used for FI, so in case of DNN where the input has a diverse range, performing FI to get meaningful resilience profile of a DNN sub-system of an ADS can be quite resource intensive, otherwise some corner case may be missed during the analysis. Keeping these things in mind Pi et al.~\cite{kexin17sosp} introduced DeepXplore, which tests deep learning systems in a systematic way by finding the inputs for DL systems, that will result in a mismatch in output for different DL systems that provide the same functionality. In this way corner cases that are not captured by regular testing and training cycle in neural networks are found, thus exposing erroneous behavior of the system without a significant overhead of finding corner cases manually. DeepXplore also finds the inputs that maximize the fraction of neurons activated during testing, thus increasing the tests coverage of DNNs. DeepXplore can provide a good defense against vulnerabilities and erroneous behaviors in DNN sub-system of ADS but for the overall reliability of the system other components of the system also need to be analyzed.

Dosovitskiy et al.~\cite{avfi} developed AVFI which is a fault injector that performs resilience assessment of complete autonomous vehicle (AV) systems. AVFI injects faults without any systematic approach in ADS which consists of CARLA simulator and a driving agent which performs the control actions. AVFI can inject hardware faults (modeling soft errors), data faults (modeling error in sensor values), timing faults and machine learning faults (errors leading to the wrong prediction).
Jha et al.~\cite{jha18art} developed Kayotee which is a tool to inject faults in an ADS (similar to AVFI) and then classifying errors and safety violations impacting the reliability of autonomous vehicles. Kayotee injects faults in both hardware and software components of proprietary Nvidia DriveWorks platform, in a closed looped environment and then error propagation and masking characteristics of the ADS compute stack are evaluated using a predefined ontology model.
Kayotee and AVFI provide a holistic view of the system but its fault coverage is also limited, as only those errors can be studied which have been simulated using FI. As the whole system is taken into consideration during holistic analysis, trying to increase the test coverage using FI 
can lead to state space explosion. We inject faults in only sensors of ADS thus avoiding state space explosion.

Work has been done on camera faults in the context of Video surveillance systems by Zhou et al. \cite{Zhou}. However, they explore the fault models of cameras with respect to video surveillance systems. This is different from ADS because in case of ADS the cameras are in constant motion so the fault model of surveillance systems is not applicable in case of ADS. Moreover, ADS are quite complex systems because they have neural networks involved in their design. This aspect makes the behavior of the system in the presence of faults different when compared to systems like video surveillance systems. For example, Goodfellow et al. \cite{Goodfellow} have shown that sometimes due to the introduction of noise, the accuracy rates of image detection in neural networks increases.